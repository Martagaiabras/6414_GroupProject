---
title: "LinearRegressionModel"
author: "Jared Babcock"
date: "11/11/2019"
output:
  pdf_document: default
  html_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
source("LoadingData.R")
source("DataTransformation.R")
```

## Initial data prep

We want to find the largest contributing factors to customer lifetime value (CLTV is our response)

Get rid of city, zip code, lat long, latitude, and longitude for now since there are too many factors (possibly add back later)

Some cities are important to predicting churn score (based on city p-value) but not all of them

There are 20 rows where total charges is na, since that is a small subset of the data (0.284%) we drop those,
it is not worth imputing and possibly adding of data


some sources:
https://stackoverflow.com/questions/7980622/subset-of-rows-containing-na-missing-values-in-a-chosen-column-of-a-data-frame
https://stackoverflow.com/questions/4605206/drop-data-frame-columns-by-name

Create full model

```{r}
model.full <- lm(`CLTV`~.,data=dat.reduced)
summary(model.full)
```

```{r}
cooks.distances <- cooks.distance(model.full)
n <- dim(dat.reduced)[1]
threshold <- 4/n

plot(cooks.distances,type='h',ylab="Cook's Distances")
abline(h=threshold,col='red')
```

We can see from this plot that we have several points with Cook's Distance above the 4/n threshold.

```{r}
above.indices <- cooks.distances[cooks.distances>threshold]

num.outliers <- length(above.indices)

num.outliers/n*100
```

2.49% of the data are outliers, so we will remove them.

```{r}
dat.nooutliers <- dat.reduced[cooks.distances<=threshold,]
```

```{r}
model.full <- lm(`CLTV`~.,data=dat.nooutliers)
model.intercept.only <- lm(`CLTV`~1,data=dat.nooutliers)

anova(model.full,model.intercept.only)
```

We can see from this ANOVA that the full model is statistically significantly better than a model with no predictors and only an intercept. So, we can do variable selection next.

```{r}
summary(model.full)
levels(dat.nooutliers$`Multiple Lines`)
```
We can see NAs present in our model summary, which indicates multicollinearity. First we will make sure that categorical variables are coded correctly. We will combine 'No' and 'No service' for multiple variables since it appears they are collinear.

```{r}
# Deal with Multiple Lines
dat.nooutliers$`Multiple Lines`[dat.nooutliers$`Multiple Lines` == 'No phone service'] <- 'No'
dat.nooutliers$`Multiple Lines` <- as.factor(dat.nooutliers$`Multiple Lines`)
dat.nooutliers$`Multiple Lines` <- droplevels(dat.nooutliers$`Multiple Lines`)

# Deal with Online Security
dat.nooutliers$`Online Security`[dat.nooutliers$`Online Security` == 'No internet service'] <- 'No'
dat.nooutliers$`Online Security` <- as.factor(dat.nooutliers$`Online Security`)
dat.nooutliers$`Online Security` <- droplevels(dat.nooutliers$`Online Security`)

# Deal with Online Backup
dat.nooutliers$`Online Backup`[dat.nooutliers$`Online Backup` == 'No internet service'] <- 'No'
dat.nooutliers$`Online Backup` <- as.factor(dat.nooutliers$`Online Backup`)
dat.nooutliers$`Online Backup` <- droplevels(dat.nooutliers$`Online Backup`)

# Deal with Device Protection
dat.nooutliers$`Device Protection`[dat.nooutliers$`Device Protection` == 'No internet service'] <- 'No'
dat.nooutliers$`Device Protection` <- as.factor(dat.nooutliers$`Device Protection`)
dat.nooutliers$`Device Protection` <- droplevels(dat.nooutliers$`Device Protection`)

# Deal with Tech Support
dat.nooutliers$`Tech Support`[dat.nooutliers$`Tech Support` == 'No internet service'] <- 'No'
dat.nooutliers$`Tech Support` <- as.factor(dat.nooutliers$`Tech Support`)
dat.nooutliers$`Tech Support` <- droplevels(dat.nooutliers$`Tech Support`)

# Deal with Streaming TV
dat.nooutliers$`Streaming TV`[dat.nooutliers$`Streaming TV` == 'No internet service'] <- 'No'
dat.nooutliers$`Streaming TV` <- as.factor(dat.nooutliers$`Streaming TV`)
dat.nooutliers$`Streaming TV` <- droplevels(dat.nooutliers$`Streaming TV`)

# Deal with Streaming Movies
dat.nooutliers$`Streaming Movies`[dat.nooutliers$`Streaming Movies` == 'No internet service'] <- 'No'
dat.nooutliers$`Streaming Movies` <- as.factor(dat.nooutliers$`Streaming Movies`)
dat.nooutliers$`Streaming Movies` <- droplevels(dat.nooutliers$`Streaming Movies`)
```

```{r}
model.full <- lm(`CLTV`~.,data=dat.nooutliers)
summary(model.full)
```

Now evaluate VIF to see if we have to remove any other collinear variables.

```{r}

vif.threshold <- 1 / (1 - summary(model.full)$r.squared)

cutoff.val <- max(10,vif.threshold)
cutoff.val
```

The variables that have GVIF > 10 exhibit multicollinearity and should be investigated.

```{r}
vif(model.full)
```

Looking at the above output, we will first remove Monthly Charges since it has a GVIF value significantly higher than the others.

```{r}
# Monthly charges is in column 18, so remove that from the data

dat.nooutliers.reduced.full <- dat.nooutliers[,-18]
```

We now want to do a 75/25 train/test split with the data for prediction later on

```{r}
## 75% of the sample size
train_size <- floor(0.75 * nrow(dat.nooutliers.reduced.full))

## set the seed to make your partition reproducible
set.seed(6414)
train_indices <- sample(seq_len(nrow(dat.nooutliers.reduced.full)), size = train_size)

testData <- dat.nooutliers.reduced.full[-train_indices, ]
dat.nooutliers.reduced <- dat.nooutliers.reduced.full[train_indices, ]
```

```{r}
model.no.collinearity <- lm(`CLTV`~.,data=dat.nooutliers.reduced)
vif(model.no.collinearity)
```

We see that removing Monthly Charges lowered the GVIF for most other variables. For now, we will keep Total Charges since it seems relevant and its GVIF is not very high above our rule of thumb threshold (and we do not always follow rules of thumb).

```{r}
sapply(dat.nooutliers.reduced,class)
```

## Model selection

Since we have so many predictors, it is not computationally feasible to do a full search for model selection

We need to scale the numeric columns Tenure Months, Total Charges, and the response (Churn Score) before regularized regression, but the glmnet function handles scaling for us.

First, use forward stepwise regression for model selection

```{r}
full <- lm(`CLTV`~.,data=dat.nooutliers.reduced)
minimum <- lm(`CLTV`~1,data=dat.nooutliers.reduced)
step(minimum, scope = list(lower=minimum, upper = full), direction = "forward",trace=FALSE)
```

Create reduced stepwise model

```{r}
model.reduced <- lm(formula = CLTV ~ `Tenure Months` + `Total Charges` + `Device Protection` + 
    `Internet Service` + `Streaming TV` + `Online Backup`, data = dat.nooutliers.reduced)
summary(model.reduced)
```


We now want to try to use the lasso method for variable selection

```{r}
# Use model.matrix to create dummy variables for lasso but drop the first column (extra intercept column)
preds <- model.matrix(as.formula(CLTV~.),data=dat.nooutliers.reduced)[,-1]

lasso.cv <- cv.glmnet(preds,dat.nooutliers.reduced$CLTV,alpha=1,nfolds=10)
lasso.lam <- lasso.cv$lambda.min 
model.lasso <- glmnet(preds,dat.nooutliers.reduced$CLTV, alpha = 1, nlambda = 100)
```

Look at the coefficients corresponding to the best lambda value

```{r}
coef(model.lasso,s=lasso.lam)
```

According to lasso regression, we want to keep Tenure Months, Internet Service, Online Security, Device Protection, Streaming TV, Contract, Payment Method, and Total Charges. Stepwise regression only kept 6 variables but lasso kept 8 variables.

```{r}
model.reduced.lasso <- lm(CLTV~`Tenure Months` + `Internet Service` + `Online Security` + `Device Protection` + `Streaming TV` + Contract
                          + `Payment Method` + `Total Charges`,data=dat.nooutliers.reduced)
summary(model.reduced.lasso)
```

We will try elastic net next since it is another variable selection technique

```{r}
elnet.cv <- cv.glmnet(preds,dat.nooutliers.reduced$CLTV,alpha=0.5,nfolds=10)
elnet.lam <- elnet.cv$lambda.min 
model.elnet <- glmnet(preds,dat.nooutliers.reduced$CLTV, alpha = 0.5, nlambda = 100)
```

Use the variables and coefficients corresponding to the model with the best lambda value

```{r}
coef(model.elnet,s=elnet.lam)
```

We want to use all variables in the elnet model, but the adjusted coefficients from elastic net

```{r}
model.not.reduced.elnet <- lm(CLTV~.,data=dat.nooutliers.reduced)
model.not.reduced.elnet$coefficients <- coef(model.elnet,s=elnet.lam)
summary(model.not.reduced.elnet)
```

Compare adjusted r^2, Mallows CP, AIC, and BIC for full, stepwise, lasso, and elastic models

```{r}
library(CombMSC)
n <- dim(dat.nooutliers.reduced)[1]
model.comp<-rbind(
  full=c(summary(model.no.collinearity)$adj.r.sq,Cp(model.no.collinearity,S2=summary(model.no.collinearity)$sigma^2),
         AIC(model.no.collinearity,k=2),AIC(model.no.collinearity,k=log(n))), 
  step=c(summary(model.reduced)$adj.r.sq,Cp(model.reduced,S2=summary(model.reduced)$sigma^2), AIC(model.reduced,k=2),
          AIC(model.reduced,k=log(n))), 
  lasso=c(summary(model.reduced.lasso)$adj.r.sq,Cp(model.reduced.lasso,S2=summary(model.reduced.lasso)$sigma^2), AIC(model.reduced.lasso,k=2),
          AIC(model.reduced.lasso,k=log(n))),
  elnet=c(summary(model.not.reduced.elnet)$adj.r.sq,Cp(model.not.reduced.elnet,S2=summary(model.not.reduced.elnet)$sigma^2),
          AIC(model.not.reduced.elnet,k=2),AIC(model.not.reduced.elnet,k=log(n)))
)
colnames(model.comp) = c("adj.rsq","Cp","AIC","BIC")
model.comp
```

The stepwise model has lowest Mallows CP, AIC, and BIC, and its adjusted R^2 is the highest of all of the models, so we select that one as our best model

## Goodness of fit

Now we want to evaluate the goodness of fit on this model

```{r}
plot(model.reduced)
```

```{r}
resids.standard <- rstandard(model.reduced)
hist(resids.standard)
```

We see from this histogram that the standardized residuals are approximately normally distributed, with a slight skew to the left side.

The QQ plot makes it appear that the model violates the Normality assumption; we have strong tails on both ends.

We see from this plot that the Constant Variance assumption also appears to be violated, since the standardized residuals seem to trend downwards as the fitted values increase. However, the Independence assumption seems to hold because we do not see separate clusters on this plot of standardized residuals vs fitted values.

We then test the Linearity of the quantitative predictors against the response. For both Tenure Months and Total Charges, we see a very dispersed relationship between each predictor and the response, indicating that the Linearity assumption does not hold.

We want to try a log transformation now to try and make the model a better fit

```{r}
sapply(dat.nooutliers.reduced,class)
```

Do a log transformation on the numeric variables Tenure Months and Total Charges

```{r}
dat.nooutliers.reduced.logtransform <- cbind(dat.nooutliers.reduced)
dat.nooutliers.reduced.logtransform$`Tenure Months` <- log(dat.nooutliers.reduced.logtransform$`Tenure Months`)
dat.nooutliers.reduced.logtransform$`Total Charges` <- log(dat.nooutliers.reduced.logtransform$`Total Charges`)

model.logpreds <- lm(CLTV ~ `Tenure Months` + `Total Charges` + `Device Protection` + 
    `Internet Service` + `Streaming TV` + `Online Backup`,data=dat.nooutliers.reduced.logtransform)
summary(model.logpreds)
```

```{r}
plot(model.logpreds)
```

```{r}
resids.standard <- rstandard(model.logpreds)
hist(resids.standard)
```

Still not a great fit with the log of predictors

Now we will try taking the log of the response without log predictors

```{r}
model.logresponse <- lm(log(CLTV) ~ `Tenure Months` + `Total Charges` + `Device Protection` + 
    `Internet Service` + `Streaming TV` + `Online Backup`,data=dat.nooutliers.reduced)
summary(model.logresponse)
```

```{r}
plot(model.logresponse)
```


```{r}
resids.standard <- rstandard(model.logresponse)
hist(resids.standard)
```

Bad normality when we transform response, slightly better linearity?

Now we take the log of both predictors and and response

```{r}
model.both.transform <- lm(log(CLTV) ~ `Tenure Months` + `Total Charges` + `Device Protection` + 
    `Internet Service` + `Streaming TV` + `Online Backup`,data=dat.nooutliers.reduced.logtransform)
summary(model.both.transform)
```

```{r}
plot(model.both.transform)
```

```{r}
resids.standard <- rstandard(model.both.transform)
hist(resids.standard)
```

Doing log transformation the response but not the predictors did slightly better in terms of linearity, so we will keep that transformation

Now we want to use the box cox transformation to try and improve normality

```{r}
bc<- boxcox(model.logresponse)

# Get lambda from the boxcox function
bc.lam <- bc$x[which.max(bc$y)]

dat.bc <- cbind(dat.nooutliers.reduced)

# Then do a power transformation using that lambda
dat.bc$CLTV <- (dat.bc$CLTV^bc.lam - 1) / bc.lam

model.bc <- lm(log(CLTV) ~ `Tenure Months` + `Total Charges` + `Device Protection` + 
    `Internet Service` + `Streaming TV` + `Online Backup`,data=dat.bc)

rs <- rstandard(model.bc)

qqnorm(rs)
qqline(rs)
```

We still have some heavy tails after the box cox transformation, but normality is slightly better

```{r}
resids.standard <- rstandard(model.bc)
hist(resids.standard)
```

We see from this histogram that the standardized residuals are approximately normally distributed, with a slight skew to the left side.

So, our final model uses this box cox transformation and the log of the response (CLTV). We still do not have a great fit to this model, but we have done what we can to improve it

## Predictions

We now want to use the testing data to evaluate prediction of our best model. Make sure to transform the response variable for testData
using the above box cox transformation for consistency

```{r}
testData.bc <- cbind(testData)

testData.bc$CLTV <- (testData.bc$CLTV^bc.lam - 1) / bc.lam

preds <- predict(model.bc,testData.bc,interval='prediction')
```

Mean squared prediction error (use log of CLTV since that is what our model predicts for)
```{r}
mean((preds-log(testData.bc$CLTV))^2)
```

Mean absolute prediction error
```{r}
mean(abs(preds-log(testData.bc$CLTV)))
```

Mean absolute percentage error
```{r}
mean(abs(preds-log(testData.bc$CLTV))/log(testData.bc$CLTV))
```

Precision measure
```{r}
sum((preds-log(testData.bc$CLTV))^2)/sum((log(testData.bc$CLTV)-mean(log(testData.bc$CLTV)))^2)
```

Since we saw that our data exhibited some nonlinear patterns, we will also evaluate prediction using a model that typically works well
with nonlinear data: gradient boosted regression. No box cox or logarithmic transformation is used here.


```{r}
library(gbm)

model.boosted <- gbm(as.formula(CLTV~.),data=dat.nooutliers.reduced,distribution = 'gaussian')

preds.boosted <- predict(model.boosted,testData,interval='prediction',n.trees=100)
```


Gradient boosted metrics
Mean squared prediction error (use non-transformed CLTV since that is what the boosted model predicts for)
```{r}
mean((preds.boosted-testData$CLTV)^2)
```

Mean absolute prediction error
```{r}
mean(abs(preds.boosted-testData$CLTV))
```

Mean absolute percentage error
```{r}
mean(abs(preds.boosted-testData$CLTV)/testData$CLTV)
```

Precision measure
```{r}
sum((preds.boosted-testData$CLTV)^2)/sum((testData$CLTV-mean(testData$CLTV))^2)
```

It appears that the gradient boosted model performs worse for prediction than our linear regression model