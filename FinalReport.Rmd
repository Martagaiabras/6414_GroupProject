---
title: '6414 Group Project - Telecom Customer Churn Modeling '
author: "Jared Babcock, Rishi Bubna, Marta Bras"
date: "`r Sys.Date()`"
output:  
  pdf_document:
    toc: true
    number_sections: true
header-includes:
    - \usepackage{setspace}\doublespacing
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
source("LoadingData.R")
source("CSS.R")
source("./Logistic Regression/Model comparison.R")
source("./Clustering/clustering.R")
```

# Abstract

The telecommunications industry is facing a challenging context, with the emergence of new technologies and the increasing levels of competition resulting in unprecedent levels of customer attrition and price comptetion between the companies in the sector. 

In a highly price sensitive market, customer service and product features play an important role. For telecommunication companies to better tailor their products to customer expectations, understanding the reasons for customer attrition is a cruccial step.  In that sense, customer churn analysis is one of the vital measures for subscription-based business models such as telecom services and internet providers.  

In this report, we developed a model to explore the reasons why a specific telecommunications company's customers churn. In our analysis, we were particularly interested in understanding differences between groups, and to leverage that information to suggest future customer segmentation strategies. We used this model to also predict which customers are likely to churn in the future.

Additionally, we use modelling to predict each customer's lifetime value (CLV), as a measure of a particular customer's net worth to the company, during his relationship with the company.

We argue that if a company is able to predict if a customer is likely to churn, while also being able to identify if the same customer is worth reatining (based on a predicted value for CLV), then the company can choose to increase engagement with the customer in order to retain him. 

Preserving "at risk" valuable customers, while leveraging on information about differences in groups to develop better segmentation strategies, can potentially help a telecommunication company differentiate from the competition and hence increase revenues in the long run.


# Introduction 

## Reasons for our analysis

Understanding customers' preferences is essential for any business, playing an even more important role when competiton and price elasticity of demand is high. This is the case for the telecommunication industry, in which customers frequently change among telecom operators, resulting in high churn rates and competitive pressures for the companies. 

Customer segmentation and targeting are marketing strategies that allow companies to differentiate by tailoring their products to different groups' preferences. In our analysis, we were interested in using modelling to understand how different groups churn and how different factors influence churn rate. We believe the information provided by our models can be used to support strategic marketing decisions of the company in the medium/long run. 

Another important aspect in strategic decision making is provided by the concept of CLV. Customer lifetime value is a prediction of the net profit of a particular customer during the future relationship with the company. In that sense, it is a good indicator of which customers are worth investing marketing efforts to retain them and which are not.In our analysis, we use modelling to predict the CLV for each customer. 

We combine our predictive model for churn with our predictive model for CLV, to provide a tool for the company to proactively identify  customers to target their marketing efforts, in an attemp to not loose them in the future. 

We believe that using analytical modelling will help telecommunications' companies ehnance their business model and marketing strategies and further differentiate from competitors.



## Project goals

Considering what was already mentioned, our goals with this project are: 

1. Building a predictive model for churn rate that best identifies which customers are likely to churn.

2. Building a predictive model for CLTV that best identifies how much a customer is worth for the company.

3. Perform customer segmentation to identify high value customers that are likely to churn.


## A Priori Expectations

We hypothesized different groups will have different churn rates and that that information might be useful for strategic decision making. We also hypothesize that it is possible to predict CLV based on demographic and product specific explanatory variables.

# Methods 

## Description of the data

The IBM Business Analytics Community provides a fictional dataset of over 7,000 customers for a telecom company that contains information about which customers have left, stayed, or signed up for their service. The dataset also contains major demographic information for customers, along with Satisfaction Score, Churn Score, and Customer Lifetime Value (CLTV) index.

The database has data from 7,043 telecom customers, all located in California (USA). The average tenure of the customers is 32 months with an average churn score (determined by the company) of 59% and an average CLTV (determined by the company) of 4,400$. 

```{r}
summary_table <- dat %>% 
  summarize(`number observations(#)` = length(CustomerID),
            `average tenure (months)` = (mean(`Tenure Months`)),
            `min tenure (months)` = min(`Tenure Months`),
            `max tenure (months)` = max(`Tenure Months`),
            `average churn score(%)` = mean(`Churn Score`),
            `min churn score(%)` = min(`Churn Score`),
            `max churn score(%)` = max(`Churn Score`),
            `average CLTV($)` = mean(CLTV),
            `min CLTV($)` = min(CLTV),
            `max CLTV($)` = max(CLTV))


summary_table <-  t(summary_table)

kable(summary_table, digits = 0, format = "latex", booktabs = T, caption = "Overview of data") %>%
  kable_styling(latex_options = c("striped", "hold_position"),
                full_width = F)
```

From the customers, 5,174 have not churned (73.4%). We categorized the different reasons for churn that were provided in the feature "Churn Reason" in the database in 5 subcategories: competitors, customer service, produce features, price and others (see table 3). Interestingly, even though competitors and price play a big role, bad customer service was reported as being the second main reason for churn. Additionally, if we consider bad customer service and bad product features together, these two reasons had a more important role in customer churn than price and competition together. 

```{r, message = FALSE}
summary_churn <- dat %>% group_by(`Churn Value`) %>%  
  summarize(`# customers` = length(`Churn Value`))



kable(summary_churn, digits = 0, format = "latex", booktabs = T, caption = "Churn vs not churn") %>%
  kable_styling(latex_options = c("striped", "hold_position"),
                full_width = F)

summary_churn_2 <- dat  %>%  filter(`Churn Reason` != "NA" & `Churn Reason` != "Don't know") %>% 
  mutate(Reason = case_when(
    grepl("Price|Extra data charges|Long distance charges",`Churn Reason`) ~ "Price", 
    grepl("Attitude|Service dissatisfaction|Poor expertise of phone support|Poor expertise of online support",`Churn Reason`) ~ "Customer service",
    grepl("Competitor",`Churn Reason`) ~ "Competitors offer",
    grepl("Network reliability|Product dissatisfaction|Lack of affordable download/upload speed|Lack of self-service on Website|Lack of affordable download/upload speed|Limited range of services",`Churn Reason`) ~ "Product features",                  
    TRUE ~ "Other"
  )) %>% 
  group_by(Reason) %>% 
  summarize(`# customers` = length(Reason)) %>% arrange(desc(`# customers`)) 



kable(summary_churn_2, digits = 0, format = "latex", booktabs = T, caption = "Top 5 churn reasons") %>%
  kable_styling(latex_options = c("striped", "hold_position"),
                full_width = F)

```

## Methods used 
The implementation and transformations performed for each of the methods used are summarized in the table below.
```{r, message = FALSE}
method  <- c("Linear Regression", "Logisitc Regression")
response  <- c("CLTV value","Churn(Y/N)")
implementation <- c("1. Splitting the data int training/testing datasets, 2. Run Full model, 3. Check for model assumptions 4. Transform the variables and remove outliers, 5. Variable selection models, 6. Performance measures in the testing dataset", "1. Splitting the data int training/testing datasets, 2. Run Full model, 3. Aggregating data for categorical variables, 4. Check for model assumptions 5. Transform the variables and remove outliers, 6. Variable selection models, 7. Performance measures in the testing dataset")


transformations <- c("1. Numerical variable to bins(categorical variable)", "1. Numerical variable to bins(categorical variable)")
a <- as.data.frame(cbind(method, response, implementation, transformations))
 

kable(a, digits =6, caption = "Results") %>%
kable_styling(latex_options = c("striped", "HOLD_position"), full_width = FALSE) %>%
column_spec(c(2, 3, 4), width = c("1.9cm","8cm", "3.5cm"))
```


# Results 

## Churn Rate - Insigths

### Insigths from clustering

As we have seen before, there are different reasons why customers churn - either because customers provided a better offer, the product features were not aligned to customers' interests, the customer service was bad, among others.


```{r, fig.align="center"}
ggplot(aes(x = X, y = Y), data = tsne_data) +
  geom_point(aes(color = cluster)) + 
  annotate(geom="text", x=0, y=34, label="Competitors",
              color="darkgreen")+ 
  annotate("rect", xmin = -8, xmax = 17, ymin = 5, ymax = 37, 
  alpha = .08, color="darkgreen", lwd = 0.2) +
  annotate(geom="text", x=-35, y=10, label="Product features",
              color="purple")+ 
  annotate("rect", xmin = -45, xmax = -20, ymin = -25, ymax = 12, 
  alpha = .08, color="purple", lwd = 0.2) +
    annotate(geom="text", x=-20, y=25, label="Customer service",
              color="turquoise4")+ 
  annotate("rect", xmin = -30, xmax = 0, ymin = -5, ymax = 28, 
  alpha = .08,  color="turquoise4", lwd = 0.2)+
    annotate(geom="text", x=-12.2, y=-8, label="Competitors",
              color="brown3")+ 
  annotate("rect", xmin = -20, xmax = 5, ymin = -35, ymax = -5, 
  alpha = .08,  color="brown3", lwd = 0.2)
```

```{r}
cluster1 <- c("100% M", "92%young", "86%No", "95%No",  "62% Fiber", "49% competitors better offer, 24% customer service" )

cluster2 <- c("100% F", "80%young", "100%No", "96%No",  "69% Fiber", "54% competitors better offer, 23% product features" )

cluster3 <- c("88% F", "86%young", "67%Yes", "91%No", "72% Fiber", "63% customer service, 16% competitors offer")

cluster4 <- c("76% M", "77%senior", "73%Yes", "95%No",  "81% Fiber", "42% product features, 18% price")

segments <- data.frame(t(cbind(cluster1,cluster2, cluster3,cluster4)))
colnames(segments) <- c("Gender (F/M)", "Age group", "Partner (Y/N)", "Dependents (Y/N)", "Main Service", "Main Reasons")

kable(segments, digits = 3, format = "latex", booktabs = T, caption = "Clustering segments") %>%
  kable_styling(latex_options = c("striped", "HOLD_position"),
                full_width = F) %>%
column_spec(c(7), width = c( "3cm"))
  
```

Understanding that different customer groups have different needs is important in defining the company's marketing strategy. 
One insigth from the clustering analysis is that, even though customer service is important for all groups, female customers might be interested in a more personalized interaction than the other groups.

Another important insigth from this clustering analysis is that the product features might be too complex for senior citizens that do not have dependents to help them successfully use the products. To address this limitation, the company can provide better assistance not only in the moment of sale, but throughout the product lifetime. Additionally, the company can also chose to develop a more basic option that is easier to use for this group of customers.

### Insights from decision tree model

```{r}
rpart.plot(fit, extra = 110)
```


## Churn Rate - Predictive model 

```{r}
#ROC curve for different models
plot(ROCRperf, col = 'darkred',lty = 2, lwd = 2, main = "ROC curve for different models", font.main=3, font.lab=3)
plot(ROCRperf_DT, col = 'darkgreen',lty = 10, lwd = 2, add = TRUE)
plot(ROCRperf_RF, col = 'orange', lty = 6, lwd = 2, add = TRUE)
#plot(ROCRperf_bayes, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
#plot(ROCRperf_svm, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
plot(ROCRperf_kknn, col = "blue", lty= 8, lwd = 2, add = TRUE)
legend(0.2,0.45, c('logistic','decistiontree','randomforest', "kknn"),lty=c(1,1),
       lwd=c(2,2),col=c('darkred','darkgreen','orange', "blue"))

```

```{r}
kable(table, digits = 3, format = "latex", booktabs = T, caption = "Results for a threshold of 0.2") %>%
  kable_styling(latex_options = c("striped", "hold_position"),
                full_width = F)

kable(table_2, digits = 3, format = "latex", booktabs = T, caption = "Results for different thresholds") %>%
  kable_styling(latex_options = c("striped", "hold_position"),
                full_width = F)

```

##  CLTV : Predictive model 




## Customer segmentation



# Discussion 
## Subject Matter Implications



## Limitations and next steps

### Limitations:

* Not all the assumptions were met when doing the goodness of fit for logisitc regression. Namely, there were discrepancies from a normal distribution.

* The time dedicated to prune the parameters in the random forest and kknn algorithms was limited. 

## Next steps:

* Improve goodness of fit for logistic model.

* Test different tunning techniques for the models.


# Attachments


```{r, fig.height=5}
ggplot(dat, aes(x=`Churn Score`)) + 
  geom_histogram(binwidth = 5, fill = "grey") +
  ggtitle("Histogram of Churn Score") +
  theme_bw() + theme(panel.border = element_blank(), panel.grid.major = element_blank(),
  panel.grid.minor = element_blank()) +
  theme(plot.title = element_text(size=11))  
```


```{r, fig.height=4}
ggplot_1 <- ggplot(dat, aes(y=`Churn Score`, x= Gender, color = Gender,  palette = "jco")) +
  geom_boxplot() +
  ggtitle("Boxplot of Gender") +
  theme_bw() + 
  theme(panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(),  axis.title.x =element_text(size=8),  axis.title.y = element_text(size=8)) +
  theme(plot.title = element_text(size=8)) 

ggplot_2 <- ggplot(dat, aes(y=`Churn Score`, x= `Senior Citizen`, color = `Senior Citizen`,  palette = "jco")) +
  geom_boxplot() +
  ggtitle("Boxplot of seniority") +
  theme_bw() + 
  theme(panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(),  axis.title.x =element_text(size=8),  axis.title.y = element_text(size=8)) +
  theme(plot.title = element_text(size=8)) 

ggplot_3 <- ggplot(dat, aes(y=`Churn Score`, x= Dependents, color = Dependents ,  palette = "jco")) +
  geom_boxplot() +
  ggtitle("Boxplot of dependents") +
  theme_bw() + 
  theme(panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(),  axis.title.x =element_text(size=8),  axis.title.y = element_text(size=8)) +
  theme(plot.title = element_text(size=8)) 

ggplot_4 <- ggplot(dat, aes(y=`Churn Score`, x=`Phone Service`, color =`Phone Service`,  palette = "jco")) +
  geom_boxplot() +
  ggtitle("Boxplot of phone service") +
  theme_bw() + 
  theme(panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(),  axis.title.x =element_text(size=8),  axis.title.y = element_text(size=8)) +
  theme(plot.title = element_text(size=8)) 

ggarrange(ggplot_1,ggplot_2, ggplot_3, ggplot_4, nrow = 2, ncol =2)
```

## Attachment I - Churn Rate 
### 1. Full model 
### 2. Model fit
### 3. Variable transformation
### 4. Re-running the model
### 5. Variable selection
### 6. Model selection

