---
title: '6414 Group Project - Telecom Customer Churn Modeling '
author: "Jared Babcock, Rishi Bubna, Marta Bras"
date: "`r Sys.Date()`"
output:
  pdf_document:
    number_sections: yes
    toc: yes
  html_document:
    df_print: paged
    toc: yes
header-includes: \usepackage{setspace}\doublespacing
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
source("LoadingData.R")
source("CSS.R")
source("./Logistic Regression/Model comparison.R")
source("./Linear Regression/LinRegCode.R")
source("./Clustering/clustering.R")
```

# Abstract

The telecommunications industry is facing a challenging future, with the emergence of new technologies and the increasing levels of competition resulting in unprecedented levels of customer attrition and price competition between the companies in the sector. 

In a highly price sensitive market, customer service and product features play an important role. For telecommunication companies to better tailor their products to customer expectations, understanding the reasons for customer attrition is a crucial step.  In that sense, customer churn analysis is one of the vital measures for subscription-based business models such as telecom services and internet providers.  

In this report, we developed a model to explore the reasons why a specific telecommunications company's customers churn. In our analysis, we were particularly interested in understanding differences between groups, and to leverage that information to suggest future customer segmentation strategies. We used this model to also predict which customers are likely to churn in the future.

Additionally, we use modelling to predict each customer's lifetime value (CLV), as a measure of a particular customer's net worth to the company, during his relationship with the company.

We argue that if a company is able to predict if a customer is likely to churn, while also being able to identify if the same customer is worth reatining (based on a predicted value for CLV), then the company can choose to increase engagement with the customer in order to retain him. 

Preserving "at risk" valuable customers, while leveraging on information about differences in groups to develop better segmentation strategies, can potentially help a telecommunication company differentiate from the competition and hence increase revenues in the long run.


# Introduction 

## Reasons for our analysis

Understanding customers' preferences is essential for any business, playing an even more important role when competition and price elasticity of demand is high. This is the case for the telecommunication industry, in which customers frequently change among telecom operators, resulting in high churn rates and competitive pressures for the companies. 

Customer segmentation and targeting are marketing strategies that allow companies to differentiate by tailoring their products to different groups' preferences. In our analysis, we were interested in using modeling to understand how different groups churn and how different factors influence churn rate. We believe the information provided by our models can be used to support strategic marketing decisions of the company in the medium/long run. 

Another important aspect in strategic decision making is provided by the concept of CLV. Customer lifetime value is a prediction of the net profit of a particular customer during the future relationship with the company. In that sense, it is a good indicator of which customers are worth investing marketing efforts to retain them and which are not.In our analysis, we use modelling to predict the CLV for each customer. 

We combine our predictive model for churn with our predictive model for CLV, to provide a tool for the company to proactively identify customers to target their marketing efforts, in an attempt to not lose them in the future. 

We believe that using analytical modeling will help telecommunications' companies ehnance their business model and marketing strategies and further differentiate from competitors.



## Project goals

Considering what was already mentioned, our goals with this project are: 

1. Building a predictive model for churn rate that best identifies which customers are likely to churn.

2. Building a predictive model for CLTV that best identifies how much a customer is worth for the company and the factors that contribute most to CLTV.

3. Perform customer segmentation to identify high value customers that are likely to churn.


## A Priori Expectations

We hypothesized different groups will have different churn rates and that that information might be useful for strategic decision making. A logistic regression model may be appropriate for predicting churn probability. We also hypothesize that a linear regression model may be useful for predicting CLV based on demographic and product specific explanatory variables.

# Methods 

## Description of the data

The IBM Business Analytics Community provides a fictional dataset of over 7,000 customers for a telecom company that contains information about which customers have left, stayed, or signed up for their service. The dataset also contains major demographic information for customers, along with Satisfaction Score, Churn Score, and Customer Lifetime Value (CLTV) index.

The database has data from 7,043 telecom customers, all located in California (USA). The average tenure of the customers is 32 months with an average churn score (determined by the company) of 59% and an average CLTV (determined by the company) of 4,400$. 

```{r}
summary_table <- dat %>% 
  summarize(`number observations(#)` = length(CustomerID),
            `average tenure (months)` = (mean(`Tenure Months`)),
            `min tenure (months)` = min(`Tenure Months`),
            `max tenure (months)` = max(`Tenure Months`),
            `average churn score(%)` = mean(`Churn Score`),
            `min churn score(%)` = min(`Churn Score`),
            `max churn score(%)` = max(`Churn Score`),
            `average CLTV($)` = mean(CLTV),
            `min CLTV($)` = min(CLTV),
            `max CLTV($)` = max(CLTV))


summary_table <-  t(summary_table)

kable(summary_table, digits = 0, format = "latex", booktabs = T, caption = "Overview of data") %>%
  kable_styling(latex_options = c("striped", "hold_position"),
                full_width = F)
```

From the customers, 5,174 have not churned (73.4%). We categorized the different reasons for churn that were provided in the feature "Churn Reason" in the database in 5 subcategories: competitors, customer service, produce features, price and others (see table 3). Interestingly, even though competitors and price play a big role, bad customer service was reported as being the second main reason for churn. Additionally, if we consider bad customer service and bad product features together, these two reasons had a more important role in customer churn than price and competition together. 

```{r, message = FALSE}
summary_churn <- dat %>% group_by(`Churn Value`) %>%  
  summarize(`# customers` = length(`Churn Value`))



kable(summary_churn, digits = 0, format = "latex", booktabs = T, caption = "Churn vs not churn") %>%
  kable_styling(latex_options = c("striped", "hold_position"),
                full_width = F)

summary_churn_2 <- dat  %>%  filter(`Churn Reason` != "NA" & `Churn Reason` != "Don't know") %>% 
  mutate(Reason = case_when(
    grepl("Price|Extra data charges|Long distance charges",`Churn Reason`) ~ "Price", 
    grepl("Attitude|Service dissatisfaction|Poor expertise of phone support|Poor expertise of online support",`Churn Reason`) ~ "Customer service",
    grepl("Competitor",`Churn Reason`) ~ "Competitors offer",
    grepl("Network reliability|Product dissatisfaction|Lack of affordable download/upload speed|Lack of self-service on Website|Lack of affordable download/upload speed|Limited range of services",`Churn Reason`) ~ "Product features",                  
    TRUE ~ "Other"
  )) %>% 
  group_by(Reason) %>% 
  summarize(`# customers` = length(Reason)) %>% arrange(desc(`# customers`)) 



kable(summary_churn_2, digits = 0, format = "latex", booktabs = T, caption = "Top 5 churn reasons") %>%
  kable_styling(latex_options = c("striped", "hold_position"),
                full_width = F)

```

## Methods used 
The implementation and transformations performed for each of the methods used are summarized in the table below.
```{r, message = FALSE}
method  <- c("Linear Regression", "Logistic Regression")
response  <- c("CLTV value","Churn(Y/N)")
implementation <- c("1. Prepare Data (remove outliers and multicollinearity), 2. Split data 75/25 into train and test sets, 3. Perform variable selection, 4. Check for model assumptions 5. Perform transformations to improve goodness of fit, 6. Interpret coefficients, 7. Create gradient boosted model, 8. Performance measures in the testing dataset", "1. Splitting the data int training/testing datasets, 2. Run Full model, 3. Aggregating data for categorical variables, 4. Check for model assumptions 5. Transform the variables and remove outliers, 6. Variable selection models, 7. Performance measures in the testing dataset")


transformations <- c("1. Combine categorical variables exhibiting multicollinearity, 2. Logarithmic transformation of response, 3. Box-Cox transformation)", "1. Numerical variable to bins(categorical variable)")
a <- as.data.frame(cbind(method, response, implementation, transformations))
 

kable(a, digits =6, caption = "Methods") %>%
kable_styling(latex_options = c("striped", "HOLD_position"), full_width = FALSE) %>%
column_spec(c(2, 3, 4), width = c("1.9cm","8cm", "3.5cm"))
```


# Results 

## Churn Rate - Insigths

### Insigths from clustering

```{r}
ggplot(aes(x = X, y = Y), data = tsne_data) +
  geom_point(aes(color = cluster))
```

```{r}
pam_results$the_summary[[1]]
```
Cluster number one is comprised of young male with no partner or dependents, that mostly have fiber optic and that churn either because a competitor made a better offer (49% of the times) or because they had a bad experience with customer service (24%).

```{r}
pam_results$the_summary[[2]]
```
Cluster number two is comprised of young feamle with no partner or dependents, that mostly have fiber optic and that churn mostly because a competitor made a better offer (54% of the times).

```{r}
pam_results$the_summary[[3]]
```

Cluster number three is comprised of young feamle with partner but no dependents, that mostly have fiber optic and that churn mostly because of customer service (63% of the times).
```{r}
pam_results$the_summary[[4]]
```
Cluster number four is comprised of senior males with partner but no dependents, that mostly have fiber optic and that churn mostly because of product features (42% of the times).

Understanding that different customer groups have different needs is important in defining the company's marketing strategy. 
One insigth from the clustering analysis is that, even though customer service is important for all groups, female customers might be interested in a more personalized interaction than the other groups.

Another important insigth from this clustering analysis is that the product features might be too complex for senior citizens that do not have dependents to help them successfully use the products. To address this limitation, the company can provide better assistance not only in the moment of sale, but throughout the product lifetime. Additionally, the company can also chose to develop a more basic option that is easier to use for this group of customers.

### Insights from decision tree model

```{r}
rpart.plot(fit, extra = 110)
```


## Churn Rate - Predictive model 

```{r}
#ROC curve for different models
plot(ROCRperf, col = 'darkred',lty = 2, lwd = 2, main = "ROC curve for different models", font.main=3, font.lab=3)
plot(ROCRperf_DT, col = 'darkgreen',lty = 10, lwd = 2, add = TRUE)
plot(ROCRperf_RF, col = 'orange', lty = 6, lwd = 2, add = TRUE)
#plot(ROCRperf_bayes, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
#plot(ROCRperf_svm, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
plot(ROCRperf_kknn, col = "blue", lty= 8, lwd = 2, add = TRUE)
legend(0.2,0.45, c('logistic','decistiontree','randomforest', "kknn"),lty=c(1,1),
       lwd=c(2,2),col=c('darkred','darkgreen','orange', "blue"))

```

```{r}
kable(table, digits = 0, format = "latex", booktabs = T, caption = "Results for a threshold of 0.2") %>%
  kable_styling(latex_options = c("striped", "hold_position"),
                full_width = F)

kable(table_2, digits = 0, format = "latex", booktabs = T, caption = "Results for different thresholds") %>%
  kable_styling(latex_options = c("striped", "hold_position"),
                full_width = F)

```

##  CLTV - Linear Regression Model

```{r}
kable(model.comp, format = "latex", booktabs = T, caption = "Metrics for different linear regression models") %>%
  kable_styling(latex_options = c("striped", "HOLD_position"),
                full_width = F)
```

The forward stepwise regression model has the best metrics for Mallow's CP, AIC, and BIC, and has only slightly smaller adjusted $R^2$ than the larger and more complex full model and elastic net model, so we will choose the forward stepwise regression model as our preferred model.

```{r}
kable(model.coefs, format = "latex", booktabs = T, caption = "Significant Predictors of CLTV") %>%
  kable_styling(latex_options = c("striped", "HOLD_position"),
                full_width = F)
```

We can see from the above table that Tenure Months, Total Charges, Device Protection, and Internet Service significantly explain CLTV at differing signficance levels. The Tenure Months coefficient can be interpreted to mean that for each additional month a customer is tenured, their lifetime value increases by ~17. The Total Charges coefficient can be interpreted to mean that for each additional dollar charge, a customer's lifetime value increases by ~0.05. The baseline for Device Protection is No protection, so that coefficient can be interpreted to mean that if a customer accepts device protection, their lifetime value decreases by ~94. The baseline for Internet Service is DSL, so that coefficient can be interpreted to mean that if a customer switches from DSL to fiber optic internet service, their lifetime value decreases by ~64. All interpretations with respect to a particular coefficient assume that all other predictors are held constant.

```{r}
kable(metrics.comp, format = "latex", booktabs = T, caption = "Comparison of Metrics for Forward Stepwise Regression Model and Boosted Regression") %>%
  kable_styling(latex_options = c("striped", "HOLD_position"),
                full_width = F)
```

We can see from various prediction metrics that the linear regression model obtained from forward stepwise regression performs signficantly better than an out-of-the-box gradient boosted regression model (gradient boosted regression further explained in the appendix). 

## Customer segmentation



# Discussion 
## Subject Matter Implications

The implications of our project are very important to any companies looking to retain important customers, not just telecommunications companies. The framework for our Linear Regression model shows that regression can be used to predict customer lifetime value using whatever data a company may have about a particular customer. The Logistic Regression section shows that companies could also produce classification models to predict the probability of a customer leaving. Both of these pieces of information are valuable on their own, but combining them could add even more value to the company. For example, a company could use classification to determine which customers are most likely to churn, then sort those customers by lifetime value (found from the other model) to determine the high-priority customers to reach out to. This information helps the company focus its retention efforts and stay profitable because the most valuable customers are retained.

## Limitations and next steps

### Limitations:

* The linearity and normality assumptions were somewhat violated in the linear regression model, even after logarithmic and box-cox transformations.

* The hyperparameters for the gradient boosted regression model was not tuned; default values were used.

* Not all the assumptions were met when doing the goodness of fit for logisitc regression. Namely, there were discrepancies from a normal distribution.

* The time dedicated to prune the parameters in the random forest and kknn algorithms was limited. 

## Next steps:

* Try additional transformations to improve goodness of fit for linear regression model.

* Spend more time finding models that fit well to the shape of our dataset.

* Improve goodness of fit for logistic model.

* Test different tunning techniques for the models.


# Appendix


```{r, fig.height=5}
ggplot(dat, aes(x=`Churn Score`)) + 
  geom_histogram(binwidth = 5, fill = "grey") +
  ggtitle("Histogram of Churn Score") +
  theme_bw() + theme(panel.border = element_blank(), panel.grid.major = element_blank(),
  panel.grid.minor = element_blank()) +
  theme(plot.title = element_text(size=11))  
```


```{r, fig.height=4}
ggplot_1 <- ggplot(dat, aes(y=`Churn Score`, x= Gender, color = Gender,  palette = "jco")) +
  geom_boxplot() +
  ggtitle("Boxplot of Gender") +
  theme_bw() + 
  theme(panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(),  axis.title.x =element_text(size=8),  axis.title.y = element_text(size=8)) +
  theme(plot.title = element_text(size=8)) 

ggplot_2 <- ggplot(dat, aes(y=`Churn Score`, x= `Senior Citizen`, color = `Senior Citizen`,  palette = "jco")) +
  geom_boxplot() +
  ggtitle("Boxplot of seniority") +
  theme_bw() + 
  theme(panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(),  axis.title.x =element_text(size=8),  axis.title.y = element_text(size=8)) +
  theme(plot.title = element_text(size=8)) 

ggplot_3 <- ggplot(dat, aes(y=`Churn Score`, x= Dependents, color = Dependents ,  palette = "jco")) +
  geom_boxplot() +
  ggtitle("Boxplot of dependents") +
  theme_bw() + 
  theme(panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(),  axis.title.x =element_text(size=8),  axis.title.y = element_text(size=8)) +
  theme(plot.title = element_text(size=8)) 

ggplot_4 <- ggplot(dat, aes(y=`Churn Score`, x=`Phone Service`, color =`Phone Service`,  palette = "jco")) +
  geom_boxplot() +
  ggtitle("Boxplot of phone service") +
  theme_bw() + 
  theme(panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(),  axis.title.x =element_text(size=8),  axis.title.y = element_text(size=8)) +
  theme(plot.title = element_text(size=8)) 

ggarrange(ggplot_1,ggplot_2, ggplot_3, ggplot_4, nrow = 2, ncol =2)
```

